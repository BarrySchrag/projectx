# -*- coding: utf-8 -*-
"""Snakecharmer - 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17dgiOZPfw7TNB7jHQgTTpsE248Ak4Gru

#  ![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Pictograms-nps-misc-rattlesnakes-2.svg/256px-Pictograms-nps-misc-rattlesnakes-2.svg.png =44x44) Magick AI - SnakeCharmer V3

**Purpose**: prototyping experiments supporting project Panto, and Akasha. Provides several video frame autoencoder architechtures from shallow and dense, to deep convolutional, temporally predictive and variational.


---


**References**: Heavily adapted from[ Keras AE Tutorial](https://blog.keras.io/building-autoencoders-in-keras.html) by [Francois Chollet](https://twitter.com/fchollet), and others where commented inline.
"""

#@title Includes
# %matplotlib inline

#from google.colab import drive
#drive.mount('/content/drive')

import sys
import numpy as np
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.python.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator

#@title Data Preparation

VIDEO_PATH = 'output-RGB.avi'
#VIDEO_PATH = 'drive/My Drive/portraitAsRGBWithFlowArrows.avi'
#VIDEO_PATH = 'drive/My Drive/portraitAsRGB.avi'

cap = cv2.VideoCapture(VIDEO_PATH)
#cap = cv2.resize(cap, (640, 360))
#Reads the frame count, width and height of the video supplied
length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print("Frames:", length)
print()

l_frames = [cap.read()[1] for i in range(length)] #discard return value of cap.read(), keep array
#print(l_frames[0]) #list of frames
print(type(l_frames), "'l_frames'")
print("items:", len(l_frames), )
print("item shape:", l_frames[0].shape, l_frames[0].dtype)
print()

frames = np.stack(l_frames, axis=0) #combine all 3d frames to a 4d tensor (batch dimension first)
#frames = frames / 255
#print(frames[0]) #tensor of frames
print(type(frames),"'frames'")
print("ndim:", frames.ndim, frames[:].shape)
print("frame shape:", frames[0].shape, frames[0].dtype)
print()

INPUT_SIZE = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*3 )
print("Frame Width:", cap.get(cv2.CAP_PROP_FRAME_WIDTH))
print("Frame Height:", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
print("Frame Size:", INPUT_SIZE)
print()

## https://stackoverflow.com/questions/41758385/resizing-images-in-keras-imagedatagenerator-flow-methods
import skimage
from skimage import transform
new_shape = (640,360,3) #drop resoltion by one factor
frames_half = np.empty(shape=(frames.shape[0],)+new_shape, dtype='float32') #coercing to float32 to prevent later autoconversion to float64
for idx in range(frames.shape[0]):
    frames_half[idx] = skimage.transform.resize(frames[idx]/frames.max(), new_shape) #normalizing to max value to improve training
print()
print("'frames'",frames.dtype, frames.shape)
print(frames[0,0])
print()
print("'frames_half'",frames_half.dtype, frames_half.shape)
print(frames_half[0,0])
print()  

##(disabled) convert values to float between zero (0) and one (1)
#print("datatype: ", frames.dtype)
#print(frames[0,0])
#print()
#if frames.dtype != "float32": 
#  frames = frames.astype('float32') / 255 #crashy method, handling this inline with training in deep models
#  print("after coercing to", frames.dtype)
#  print(frames[0,0])
#  print()
#else:
#  print("already float32, no conversion applied")
#print()

##(disabled) flatten images for Dense (non-conv)
#x_train = frames.reshape(length,-1) #2d array (flattened image, frames)
#x_train.shape
#print(x_train[0])
#print("obj 'x_train'", x_train.shape)
#print("obj 'x_train' frame", x_train[0].shape)
#print()

#@title Shallow Convolutional AE Model from SnakeCharmer V2

frames_in = frames
#frames_in = frames_half
encoding_dim = 128 #encoder compressed vector length
#encoding_dim = (16, 16, 16)
kernel_size = (3, 3)

## define keras model
#input_img = Input(shape=(INPUT_SIZE,))
#input_img = Input(shape=(720, 1280, 3))
input_img = Input(shape=frames_in[0].shape)

# "encoded" is the encoded representation of the input
#encoded = Dense(encoding_dim, activation='relu')(input_img)
encoded = Conv2D(filters = encoding_dim, 
                 kernel_size = kernel_size, 
                 strides = (1, 1), 
                 padding = 'same', 
                 data_format = "channels_last", 
                 activation = 'relu'
                )(input_img)

# "decoded" is the lossy reconstruction of the input
#decoded = Dense(INPUT_SIZE, activation='sigmoid')(encoded)
decoded = Conv2DTranspose(frames_in.shape[3], 
                          kernel_size, 
                          strides = (1, 1), 
                          padding = 'same', 
                          data_format = "channels_last", 
                          activation = 'sigmoid'
                         )(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
#encoded_input = Input(shape=(encoding_dim,))
# retrieve the last layer of the autoencoder model
#decoder_layer = autoencoder.layers[-1]
# create the decoder model
#decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer = 'adadelta', 
                    loss = 'binary_crossentropy')

print(autoencoder.summary())

#autoencoder.fit(x_train, x_train,
autoencoder.fit(frames_in, 
                frames_in,
                epochs = 5,
                batch_size = 16,
                shuffle = True)

# %matplotlib inline

frame_num = 11

plt.imshow(frames_half[frame_num] * 1)
plt.show()

out_frames = autoencoder.predict(frames_half[frame_num:frame_num + 1], 
                                 batch_size = None, 
                                 verbose = 0,  
                                 steps = None)
  
plt.imshow(out_frames[0] * 1) #255 blows out of range probably due to normalization
plt.show()
  
display(out_frames[0] * 1)

#@title Shallow Convolutional AE with Data Augmentation { output-height: 11, display-mode: "both" }

## Model vars
encoding_dim = 128 #encoder compressed vector length
kernel_size = (3, 3)
strides = (2, 2)

## Training vars
batch_size = 8
epochs = 10
#frames_in = frames
frames_in = frames_half

## define keras model
#input_img = Input(shape=(INPUT_SIZE,))
#input_img = Input(shape=(720, 1280, 3))
input_img = Input(shape = frames_in[0].shape)

## "encoded" is the encoded representation of the input
#encoded = Dense(encoding_dim, activation='relu')(input_img)
encoded = Conv2D(filters = encoding_dim, 
                 kernel_size = kernel_size, 
                 strides = strides, 
                 padding = 'same', 
                 data_format = "channels_last", 
                 activation = 'relu'
                )(input_img)

## "decoded" is the lossy reconstruction of the input
#decoded = Dense(INPUT_SIZE, activation='sigmoid')(encoded)
decoded = Conv2DTranspose(frames_in.shape[3], 
                          kernel_size = kernel_size, 
                          strides = strides, 
                          padding = 'same', 
                          data_format = "channels_last", 
                          activation = 'sigmoid'
                         )(encoded)

## this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

## this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

## create a placeholder for an encoded (32-dimensional) input
#encoded_input = Input(shape=(encoding_dim,))
## retrieve the last layer of the autoencoder model
#decoder_layer = autoencoder.layers[-1]
## create the decoder model
#decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')
print(autoencoder.summary())

## data augmentation and batch normalization - inline with training
train_datagen = ImageDataGenerator(rotation_range = 20,
                                   width_shift_range =  0.2,
                                   height_shift_range = 0.2,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True,
                                   featurewise_center = True,
                                   featurewise_std_normalization = True,
                                  )

#frames = train_datagen.standardize(frames) #crashy method won't work with uint8, throws...
#TypeError: Cannot cast ufunc multiply output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'
train_datagen.fit(frames_in) #required for featurewise_center

#autoencoder.fit(frames, frames, batch_size=4, epochs=2, shuffle=True)
autoencoder.fit(train_datagen.flow(frames_in, 
                                   frames_in, 
                                   batch_size = batch_size), 
                steps_per_epoch = len(frames_in) / batch_size, 
                epochs = epochs, 
                shuffle = True
               )

autoencoder.fit(train_datagen.flow(frames_in, 
                                   frames_in, 
                                   batch_size = batch_size), 
                steps_per_epoch = len(frames_in) / batch_size, 
                epochs = 50, 
                shuffle = True
               )

# %matplotlib inline

#for i in range(1, frames.shape[0]): #return all frames
for i in range(1, 4): #only return range
  f, axarr = plt.subplots(1, 2)
  axarr[0].imshow(frames_in[i] * 1)
  out_frames = autoencoder.predict(frames_in[i : i + 1], 
                                   batch_size = None, 
                                   verbose = 0, 
                                   steps = None)
  axarr[1].imshow(out_frames[0] * 1)
  plt.show()

#@title Deep Convolutional AE

## Training vars
epochs = 20
batch_size = 32
#frames_in = frames
frames_in = frames_half

## Model vars
kernel_size = (3, 3)
strides = (2, 2)
filters = 32

## define keras model
#input_img = Input(shape=(720, 1280, 3))
#input_img = Input(shape=(360, 640, 3))
input_img = Input(shape=frames_in[0].shape)

x = Conv2D(filters * 2, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(input_img)
x = MaxPooling2D(strides, data_format = "channels_last", padding = 'same')(x)
x = Conv2D(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
x = MaxPooling2D(strides, data_format = "channels_last", padding = 'same')(x)
x = Conv2D(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
x = MaxPooling2D((5, 5), data_format = "channels_last", padding = 'same')(x)
x = Conv2D(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
encoded = MaxPooling2D(strides, data_format = "channels_last", padding = 'same')(x)

## at this point the representation is (9, 16, filters)

x = Conv2DTranspose(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(encoded)
x = UpSampling2D(strides, data_format = "channels_last")(x)
x = Conv2DTranspose(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
x = UpSampling2D((5, 5), data_format = "channels_last")(x)
x = Conv2DTranspose(filters, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
x = UpSampling2D(strides, data_format = "channels_last")(x)
x = Conv2DTranspose(filters * 2, kernel_size, data_format = "channels_last", activation = 'relu', padding = 'same')(x)
x = UpSampling2D(strides, data_format = "channels_last")(x)
decoded = Conv2DTranspose(frames_in.shape[3], kernel_size, data_format = "channels_last", activation = 'sigmoid', padding = 'same')(x)

## this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

## this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

## create a placeholder for an encoded (32-dimensional) input
#encoded_input = Input(shape=(encoding_dim,))
## retrieve the last layer of the autoencoder model
#decoder_layer = autoencoder.layers[-1]
## create the decoder model
#decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')
print(autoencoder.summary())

## data augmentation and batch normalization - inline with training
train_datagen = ImageDataGenerator(rotation_range = 20,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True,
                                   featurewise_center = True, 
                                   featurewise_std_normalization = True,
                                  )

#frames = train_datagen.standardize(frames) #crashy method won't work with uint8, throws...
#TypeError: Cannot cast ufunc multiply output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'
train_datagen.fit(frames_in) #required for featurewise_center

#autoencoder.fit(frames, frames, batch_size=4, epochs=2, shuffle=True)
autoencoder.fit(train_datagen.flow(frames_in,                 
                                   frames_in, 
                                   batch_size = batch_size), 
                steps_per_epoch = len(frames_in) / batch_size, 
                epochs = epochs, 
                shuffle = True
               )

autoencoder.fit(train_datagen.flow(frames_in, 
                                   frames_in, 
                                   batch_size = batch_size), 
                steps_per_epoch = len(frames_in) / batch_size, 
                epochs = 100, 
                shuffle = True
               )

# %matplotlib inline

begin = 1
end = 5

#for i in range(1, frames_in.shape[0]): #return all frames
for i in range(begin, end + 1): #only return range
  f, axarr = plt.subplots(1, 2)
  axarr[0].imshow(frames_in[i] * 1)
  out_frames = autoencoder.predict(frames_in[i : i + 1], 
                                   batch_size = None, 
                                   verbose = 0, 
                                   steps = None)
  axarr[1].imshow(out_frames[0] * 1)
  print("frame number: ", i)
  plt.show()

## We can also have a look at the 128-dimensional encoded representations. 
## These representations are 8x4x4, so we reshape them to 4x32 
## in order to be able to display them as grayscale images

n=11

plt.figure(figsize=(n, 8))
for i in range(1, n + 1):
  ax = plt.subplot(1, n, i)
  encoded = encoder.predict(frames_in[i : i + 1], 
                            batch_size = None, 
                            verbose = 0, 
                            steps = None)
  encoded.shape[3]
  plt.imshow(encoded.reshape(encoded.shape[1], encoded.shape[2]*encoded.shape[3]).T)
  plt.gray
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
print("Visualizing Latent Representations from first", n, "frames")
plt.show()
  
  
print(encoder.summary())

"""**In Progress:** below"""

## Train with intermediate save points with count of aggregate epochs

## Integrate training analytics

#tensorboard --logdir=/tmp/autoencoder

#https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6
#https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab
#https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/

## Here's how we will generate synthetic noisy digits: we just apply 
## a gaussian noise matrix and clip the images between 0 and 1.

#noise_factor = 0.5
#x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
#x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

#x_train_noisy = np.clip(x_train_noisy, 0., 1.)
#x_test_noisy = np.clip(x_test_noisy, 0., 1.)

## Here's what the noisy digits look like:

#n = 10
#plt.figure(figsize=(20, 2))
#for i in range(n):
#    ax = plt.subplot(1, n, i)
#    plt.imshow(x_test_noisy[i].reshape(28, 28))
#    plt.gray()
#    ax.get_xaxis().set_visible(False)
#    ax.get_yaxis().set_visible(False)
#plt.show()



### Temporal Auto Encoder - Sequence-to-sequence autoencoder from F Chollet Tutorial

## If you inputs are sequences, rather than vectors or 2D images, 
## then you may want to use as encoder and decoder a type of model 
## that can capture temporal structure, such as a LSTM. To build a 
## LSTM-based autoencoder, first use a LSTM encoder to turn your 
## input sequences into a single vector that contains information about 
## the entire sequence, then repeat this vector n times 
## (where n is the number of timesteps in the output sequence), and run 
## a LSTM decoder to turn this constant sequence into the target sequence.

#from keras.layers import Input, LSTM, RepeatVector
#from keras.models import Model

#seq_inputs = Input(shape=(timesteps, input_dim))
#seq_encoded = LSTM(latent_dim)(seq_inputs)

#seq_decoded = RepeatVector(timesteps)(seq_encoded)
#seq_decoded = LSTM(input_dim, return_sequences=True)(seq_decoded)

#seq_autoencoder = Model(seq_inputs, seq_decoded)
#seq_encoder = Model(seq_inputs, seq_encoded)

### VAE Example

## Github as a standalone script
## https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py

## replace Conv2D with ConvLSTM2D or gruCNN 
## or some other encoding arch alternative to Classic ConvNets

## VAE with reduced images

### Test-Train split the frames for measurement of transfer learning

### Massive Pretraining

### Add Classifier for benchmark datasets